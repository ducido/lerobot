{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.configs.eval import EvalPipelineConfig\n",
    "from lerobot.configs.default import EvalConfig\n",
    "from lerobot.envs.configs import LiberoEnv\n",
    "from lerobot.policies.pi05.configuration_pi05 import PI05Config\n",
    "from lerobot.policies.factory import make_policy, make_pre_post_processors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lerobot.configs.policies:Device 'None' is not available. Switching to 'cuda'.\n",
      "WARNING:lerobot.configs.eval:No pretrained path was provided, evaluated policy will be built from scratch (random weights).\n"
     ]
    }
   ],
   "source": [
    "cfg = EvalPipelineConfig(\n",
    "    env=LiberoEnv(),\n",
    "    eval=EvalConfig(\n",
    "        batch_size=1,\n",
    "        n_episodes=1\n",
    "    ),\n",
    "    policy=PI05Config(n_action_steps=10),\n",
    "    output_dir='./libero_eval_logs',\n",
    "    job_name='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalConfig(n_episodes=1, batch_size=1, use_async_envs=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp train_config.json /mnt/lustre-grete/usr/u12045/vla/hf_cache/hub/models--lerobot--pi0/snapshots/8f50aacbe079a026391616cf22453de528f2a873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lerobot.common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlerobot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dataset\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlerobot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EpisodeAwareSampler\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlerobot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycle\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lerobot.common'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/VLA/hf_cache\"\n",
    "os.environ[\"PATH\"] = \"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/miniconda3/envs/pitorch/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTHONPATH\"] = \"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/VLA/duc/VLA-Humanoid:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "import sys\n",
    "sys.path.insert(0, \"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/VLA/duc/VLA-Humanoid\")\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HF_HOME\"] = \"/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/VLA/hf_cache\"\n",
    "# os.environ[\"PATH\"] = \"/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"PYTHONPATH\"] = \"/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/VLA/duc3/VLA-Humanoid:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "# import sys\n",
    "# sys.path.insert(0, \"/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/VLA/duc3/VLA-Humanoid\")\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from pprint import pformat\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from termcolor import colored\n",
    "from torch.amp import GradScaler\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "from lerobot.common.datasets.sampler import EpisodeAwareSampler\n",
    "from lerobot.common.datasets.utils import cycle\n",
    "from lerobot.common.envs.factory import make_env\n",
    "from lerobot.common.optim.factory import make_optimizer_and_scheduler\n",
    "from lerobot.common.policies.factory import make_policy\n",
    "from lerobot.common.policies.pretrained import PreTrainedPolicy\n",
    "from lerobot.common.policies.utils import get_device_from_parameters\n",
    "from lerobot.common.utils.logging_utils import AverageMeter, MetricsTracker\n",
    "from lerobot.common.utils.random_utils import set_seed\n",
    "from lerobot.common.utils.train_utils import (\n",
    "    get_step_checkpoint_dir,\n",
    "    get_step_identifier,\n",
    "    load_training_state,\n",
    "    save_checkpoint,\n",
    "    update_last_checkpoint,\n",
    ")\n",
    "from lerobot.common.utils.utils import (\n",
    "    format_big_number,\n",
    "    get_safe_torch_device,\n",
    "    has_method,\n",
    "    init_logging,\n",
    ")\n",
    "from lerobot.common.utils.wandb_utils import WandBLogger\n",
    "from lerobot.configs import parser\n",
    "from lerobot.configs.train import TrainPipelineConfig\n",
    "from lerobot.scripts.eval import eval_policy\n",
    "from lerobot.common.constants import ACTION, OBS_STATE\n",
    "\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.configs.policies import PreTrainedConfig\n",
    "from lerobot.configs.default import DatasetConfig, EvalConfig, WandBConfig\n",
    "from lerobot.common.datasets.transforms import ImageTransformsConfig\n",
    "\n",
    "with open(\"../configs/libero_config/default.json\") as f:\n",
    "    data = json.load(f)\n",
    "# /mnt/lustre-grete/usr/u12045/vla/hf_cache/lerobot/binhng/libero_spatial_mask_debug_noops_lerobot\n",
    "cfg = TrainPipelineConfig(\n",
    "    policy=PreTrainedConfig.from_pretrained(\"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/VLA/temp/baseline\"),\n",
    "    dataset=DatasetConfig(\n",
    "        repo_id=\"None\",\n",
    "        root=\"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/VLA/LIBERO/merged_libero_scale_10_mask_depth_noops_lerobot_original\",\n",
    "        image_transforms=ImageTransformsConfig.from_dict(data[\"dataset\"]['image_transforms']),\n",
    "    ),\n",
    "    wandb=WandBConfig(\n",
    "        project=\"lerobot\",\n",
    "        entity=\"lerobot\",\n",
    "    ),\n",
    ")\n",
    "cfg.validate()\n",
    "# logging.info(pformat(cfg.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.policy.chunk_size = 10\n",
    "cfg.policy.n_action_steps = 10\n",
    "cfg.batch_size = 1\n",
    "cfg.wandb.enable = False\n",
    "cfg.dataset.image_transforms.enable = True\n",
    "cfg.log_freq = 20\n",
    "cfg.num_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-30 13:01:30 2197283937.py:5 \u001b[1m\u001b[33mLogs will be saved locally.\u001b[0m\n",
      "INFO 2025-12-30 13:01:30 197283937.py:14 Creating dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969a0a2f59614ce3937bfd90c3c63aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709f2849f5d645f390bd5ca5a7622996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/200 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd54532376245cea8f7b4b73d22b175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'episode_index': 0, 'tasks': ['put the black bowl in the bottom drawer of the cabinet and close it'], 'length': 247}, 1: {'episode_index': 1, 'tasks': ['put the black bowl in the bottom drawer of the cabinet and close it'], 'length': 248}, 2: {'episode_index': 2, 'tasks': ['put the black bowl in the bottom drawer of the cabinet and close it'], 'length': 235}, 3: {'episode_index': 3, 'tasks': ['put the black bowl in the bottom drawer of the cabinet and close it'], 'length': 231}, 4: {'episode_index': 4, 'tasks': ['put the black bowl in the bottom drawer of the cabinet and close it'], 'length': 223}, 5: {'episode_index': 5, 'tasks': ['put the yellow and white mug in the microwave and close it'], 'length': 449}, 6: {'episode_index': 6, 'tasks': ['put the yellow and white mug in the microwave and close it'], 'length': 265}, 7: {'episode_index': 7, 'tasks': ['put the yellow and white mug in the microwave and close it'], 'length': 248}, 8: {'episode_index': 8, 'tasks': ['put the yellow and white mug in the microwave and close it'], 'length': 288}, 9: {'episode_index': 9, 'tasks': ['put the yellow and white mug in the microwave and close it'], 'length': 345}, 10: {'episode_index': 10, 'tasks': ['put both the alphabet soup and the cream cheese box in the basket'], 'length': 261}, 11: {'episode_index': 11, 'tasks': ['put both the alphabet soup and the cream cheese box in the basket'], 'length': 219}, 12: {'episode_index': 12, 'tasks': ['put both the alphabet soup and the cream cheese box in the basket'], 'length': 288}, 13: {'episode_index': 13, 'tasks': ['put both the alphabet soup and the cream cheese box in the basket'], 'length': 255}, 14: {'episode_index': 14, 'tasks': ['put both the alphabet soup and the cream cheese box in the basket'], 'length': 312}, 15: {'episode_index': 15, 'tasks': ['pick up the book and place it in the back compartment of the caddy'], 'length': 170}, 16: {'episode_index': 16, 'tasks': ['pick up the book and place it in the back compartment of the caddy'], 'length': 162}, 17: {'episode_index': 17, 'tasks': ['pick up the book and place it in the back compartment of the caddy'], 'length': 189}, 18: {'episode_index': 18, 'tasks': ['pick up the book and place it in the back compartment of the caddy'], 'length': 259}, 19: {'episode_index': 19, 'tasks': ['pick up the book and place it in the back compartment of the caddy'], 'length': 244}, 20: {'episode_index': 20, 'tasks': ['put both the alphabet soup and the tomato sauce in the basket'], 'length': 285}, 21: {'episode_index': 21, 'tasks': ['put both the alphabet soup and the tomato sauce in the basket'], 'length': 259}, 22: {'episode_index': 22, 'tasks': ['put both the alphabet soup and the tomato sauce in the basket'], 'length': 245}, 23: {'episode_index': 23, 'tasks': ['put both the alphabet soup and the tomato sauce in the basket'], 'length': 296}, 24: {'episode_index': 24, 'tasks': ['put both the alphabet soup and the tomato sauce in the basket'], 'length': 250}, 25: {'episode_index': 25, 'tasks': ['put the white mug on the left plate and put the yellow and white mug on the right plate'], 'length': 225}, 26: {'episode_index': 26, 'tasks': ['put the white mug on the left plate and put the yellow and white mug on the right plate'], 'length': 233}, 27: {'episode_index': 27, 'tasks': ['put the white mug on the left plate and put the yellow and white mug on the right plate'], 'length': 233}, 28: {'episode_index': 28, 'tasks': ['put the white mug on the left plate and put the yellow and white mug on the right plate'], 'length': 253}, 29: {'episode_index': 29, 'tasks': ['put the white mug on the left plate and put the yellow and white mug on the right plate'], 'length': 279}, 30: {'episode_index': 30, 'tasks': ['turn on the stove and put the moka pot on it'], 'length': 275}, 31: {'episode_index': 31, 'tasks': ['turn on the stove and put the moka pot on it'], 'length': 248}, 32: {'episode_index': 32, 'tasks': ['turn on the stove and put the moka pot on it'], 'length': 267}, 33: {'episode_index': 33, 'tasks': ['turn on the stove and put the moka pot on it'], 'length': 299}, 34: {'episode_index': 34, 'tasks': ['turn on the stove and put the moka pot on it'], 'length': 278}, 35: {'episode_index': 35, 'tasks': ['put both moka pots on the stove'], 'length': 388}, 36: {'episode_index': 36, 'tasks': ['put both moka pots on the stove'], 'length': 436}, 37: {'episode_index': 37, 'tasks': ['put both moka pots on the stove'], 'length': 466}, 38: {'episode_index': 38, 'tasks': ['put both moka pots on the stove'], 'length': 362}, 39: {'episode_index': 39, 'tasks': ['put both moka pots on the stove'], 'length': 450}, 40: {'episode_index': 40, 'tasks': ['put the white mug on the plate and put the chocolate pudding to the right of the plate'], 'length': 264}, 41: {'episode_index': 41, 'tasks': ['put the white mug on the plate and put the chocolate pudding to the right of the plate'], 'length': 232}, 42: {'episode_index': 42, 'tasks': ['put the white mug on the plate and put the chocolate pudding to the right of the plate'], 'length': 247}, 43: {'episode_index': 43, 'tasks': ['put the white mug on the plate and put the chocolate pudding to the right of the plate'], 'length': 237}, 44: {'episode_index': 44, 'tasks': ['put the white mug on the plate and put the chocolate pudding to the right of the plate'], 'length': 237}, 45: {'episode_index': 45, 'tasks': ['put both the cream cheese box and the butter in the basket'], 'length': 246}, 46: {'episode_index': 46, 'tasks': ['put both the cream cheese box and the butter in the basket'], 'length': 247}, 47: {'episode_index': 47, 'tasks': ['put both the cream cheese box and the butter in the basket'], 'length': 254}, 48: {'episode_index': 48, 'tasks': ['put both the cream cheese box and the butter in the basket'], 'length': 252}, 49: {'episode_index': 49, 'tasks': ['put both the cream cheese box and the butter in the basket'], 'length': 255}, 50: {'episode_index': 50, 'tasks': ['put the bowl on the plate'], 'length': 90}, 51: {'episode_index': 51, 'tasks': ['put the bowl on the plate'], 'length': 87}, 52: {'episode_index': 52, 'tasks': ['put the bowl on the plate'], 'length': 98}, 53: {'episode_index': 53, 'tasks': ['put the bowl on the plate'], 'length': 95}, 54: {'episode_index': 54, 'tasks': ['put the bowl on the plate'], 'length': 89}, 55: {'episode_index': 55, 'tasks': ['put the wine bottle on the rack'], 'length': 160}, 56: {'episode_index': 56, 'tasks': ['put the wine bottle on the rack'], 'length': 216}, 57: {'episode_index': 57, 'tasks': ['put the wine bottle on the rack'], 'length': 196}, 58: {'episode_index': 58, 'tasks': ['put the wine bottle on the rack'], 'length': 162}, 59: {'episode_index': 59, 'tasks': ['put the wine bottle on the rack'], 'length': 179}, 60: {'episode_index': 60, 'tasks': ['put the cream cheese in the bowl'], 'length': 110}, 61: {'episode_index': 61, 'tasks': ['put the cream cheese in the bowl'], 'length': 95}, 62: {'episode_index': 62, 'tasks': ['put the cream cheese in the bowl'], 'length': 113}, 63: {'episode_index': 63, 'tasks': ['put the cream cheese in the bowl'], 'length': 110}, 64: {'episode_index': 64, 'tasks': ['put the cream cheese in the bowl'], 'length': 111}, 65: {'episode_index': 65, 'tasks': ['open the top drawer and put the bowl inside'], 'length': 219}, 66: {'episode_index': 66, 'tasks': ['open the top drawer and put the bowl inside'], 'length': 188}, 67: {'episode_index': 67, 'tasks': ['open the top drawer and put the bowl inside'], 'length': 177}, 68: {'episode_index': 68, 'tasks': ['open the top drawer and put the bowl inside'], 'length': 185}, 69: {'episode_index': 69, 'tasks': ['open the top drawer and put the bowl inside'], 'length': 190}, 70: {'episode_index': 70, 'tasks': ['push the plate to the front of the stove'], 'length': 149}, 71: {'episode_index': 71, 'tasks': ['push the plate to the front of the stove'], 'length': 118}, 72: {'episode_index': 72, 'tasks': ['push the plate to the front of the stove'], 'length': 139}, 73: {'episode_index': 73, 'tasks': ['push the plate to the front of the stove'], 'length': 215}, 74: {'episode_index': 74, 'tasks': ['push the plate to the front of the stove'], 'length': 147}, 75: {'episode_index': 75, 'tasks': ['open the middle drawer of the cabinet'], 'length': 128}, 76: {'episode_index': 76, 'tasks': ['open the middle drawer of the cabinet'], 'length': 127}, 77: {'episode_index': 77, 'tasks': ['open the middle drawer of the cabinet'], 'length': 126}, 78: {'episode_index': 78, 'tasks': ['open the middle drawer of the cabinet'], 'length': 142}, 79: {'episode_index': 79, 'tasks': ['open the middle drawer of the cabinet'], 'length': 128}, 80: {'episode_index': 80, 'tasks': ['put the bowl on top of the cabinet'], 'length': 98}, 81: {'episode_index': 81, 'tasks': ['put the bowl on top of the cabinet'], 'length': 112}, 82: {'episode_index': 82, 'tasks': ['put the bowl on top of the cabinet'], 'length': 93}, 83: {'episode_index': 83, 'tasks': ['put the bowl on top of the cabinet'], 'length': 123}, 84: {'episode_index': 84, 'tasks': ['put the bowl on top of the cabinet'], 'length': 98}, 85: {'episode_index': 85, 'tasks': ['put the wine bottle on top of the cabinet'], 'length': 114}, 86: {'episode_index': 86, 'tasks': ['put the wine bottle on top of the cabinet'], 'length': 97}, 87: {'episode_index': 87, 'tasks': ['put the wine bottle on top of the cabinet'], 'length': 102}, 88: {'episode_index': 88, 'tasks': ['put the wine bottle on top of the cabinet'], 'length': 99}, 89: {'episode_index': 89, 'tasks': ['put the wine bottle on top of the cabinet'], 'length': 113}, 90: {'episode_index': 90, 'tasks': ['turn on the stove'], 'length': 96}, 91: {'episode_index': 91, 'tasks': ['turn on the stove'], 'length': 81}, 92: {'episode_index': 92, 'tasks': ['turn on the stove'], 'length': 97}, 93: {'episode_index': 93, 'tasks': ['turn on the stove'], 'length': 92}, 94: {'episode_index': 94, 'tasks': ['turn on the stove'], 'length': 77}, 95: {'episode_index': 95, 'tasks': ['put the bowl on the stove'], 'length': 91}, 96: {'episode_index': 96, 'tasks': ['put the bowl on the stove'], 'length': 90}, 97: {'episode_index': 97, 'tasks': ['put the bowl on the stove'], 'length': 93}, 98: {'episode_index': 98, 'tasks': ['put the bowl on the stove'], 'length': 114}, 99: {'episode_index': 99, 'tasks': ['put the bowl on the stove'], 'length': 102}, 100: {'episode_index': 100, 'tasks': ['pick up the black bowl next to the plate and place it on the plate'], 'length': 121}, 101: {'episode_index': 101, 'tasks': ['pick up the black bowl next to the plate and place it on the plate'], 'length': 114}, 102: {'episode_index': 102, 'tasks': ['pick up the black bowl next to the plate and place it on the plate'], 'length': 117}, 103: {'episode_index': 103, 'tasks': ['pick up the black bowl next to the plate and place it on the plate'], 'length': 115}, 104: {'episode_index': 104, 'tasks': ['pick up the black bowl next to the plate and place it on the plate'], 'length': 173}, 105: {'episode_index': 105, 'tasks': ['pick up the black bowl between the plate and the ramekin and place it on the plate'], 'length': 89}, 106: {'episode_index': 106, 'tasks': ['pick up the black bowl between the plate and the ramekin and place it on the plate'], 'length': 93}, 107: {'episode_index': 107, 'tasks': ['pick up the black bowl between the plate and the ramekin and place it on the plate'], 'length': 168}, 108: {'episode_index': 108, 'tasks': ['pick up the black bowl between the plate and the ramekin and place it on the plate'], 'length': 84}, 109: {'episode_index': 109, 'tasks': ['pick up the black bowl between the plate and the ramekin and place it on the plate'], 'length': 93}, 110: {'episode_index': 110, 'tasks': ['pick up the black bowl next to the cookie box and place it on the plate'], 'length': 122}, 111: {'episode_index': 111, 'tasks': ['pick up the black bowl next to the cookie box and place it on the plate'], 'length': 106}, 112: {'episode_index': 112, 'tasks': ['pick up the black bowl next to the cookie box and place it on the plate'], 'length': 129}, 113: {'episode_index': 113, 'tasks': ['pick up the black bowl next to the cookie box and place it on the plate'], 'length': 146}, 114: {'episode_index': 114, 'tasks': ['pick up the black bowl next to the cookie box and place it on the plate'], 'length': 113}, 115: {'episode_index': 115, 'tasks': ['pick up the black bowl on the wooden cabinet and place it on the plate'], 'length': 182}, 116: {'episode_index': 116, 'tasks': ['pick up the black bowl on the wooden cabinet and place it on the plate'], 'length': 139}, 117: {'episode_index': 117, 'tasks': ['pick up the black bowl on the wooden cabinet and place it on the plate'], 'length': 138}, 118: {'episode_index': 118, 'tasks': ['pick up the black bowl on the wooden cabinet and place it on the plate'], 'length': 125}, 119: {'episode_index': 119, 'tasks': ['pick up the black bowl on the wooden cabinet and place it on the plate'], 'length': 141}, 120: {'episode_index': 120, 'tasks': ['pick up the black bowl on the cookie box and place it on the plate'], 'length': 96}, 121: {'episode_index': 121, 'tasks': ['pick up the black bowl on the cookie box and place it on the plate'], 'length': 97}, 122: {'episode_index': 122, 'tasks': ['pick up the black bowl on the cookie box and place it on the plate'], 'length': 99}, 123: {'episode_index': 123, 'tasks': ['pick up the black bowl on the cookie box and place it on the plate'], 'length': 99}, 124: {'episode_index': 124, 'tasks': ['pick up the black bowl on the cookie box and place it on the plate'], 'length': 92}, 125: {'episode_index': 125, 'tasks': ['pick up the black bowl from table center and place it on the plate'], 'length': 110}, 126: {'episode_index': 126, 'tasks': ['pick up the black bowl from table center and place it on the plate'], 'length': 105}, 127: {'episode_index': 127, 'tasks': ['pick up the black bowl from table center and place it on the plate'], 'length': 138}, 128: {'episode_index': 128, 'tasks': ['pick up the black bowl from table center and place it on the plate'], 'length': 122}, 129: {'episode_index': 129, 'tasks': ['pick up the black bowl from table center and place it on the plate'], 'length': 121}, 130: {'episode_index': 130, 'tasks': ['pick up the black bowl next to the ramekin and place it on the plate'], 'length': 141}, 131: {'episode_index': 131, 'tasks': ['pick up the black bowl next to the ramekin and place it on the plate'], 'length': 137}, 132: {'episode_index': 132, 'tasks': ['pick up the black bowl next to the ramekin and place it on the plate'], 'length': 150}, 133: {'episode_index': 133, 'tasks': ['pick up the black bowl next to the ramekin and place it on the plate'], 'length': 114}, 134: {'episode_index': 134, 'tasks': ['pick up the black bowl next to the ramekin and place it on the plate'], 'length': 112}, 135: {'episode_index': 135, 'tasks': ['pick up the black bowl on the stove and place it on the plate'], 'length': 140}, 136: {'episode_index': 136, 'tasks': ['pick up the black bowl on the stove and place it on the plate'], 'length': 128}, 137: {'episode_index': 137, 'tasks': ['pick up the black bowl on the stove and place it on the plate'], 'length': 126}, 138: {'episode_index': 138, 'tasks': ['pick up the black bowl on the stove and place it on the plate'], 'length': 141}, 139: {'episode_index': 139, 'tasks': ['pick up the black bowl on the stove and place it on the plate'], 'length': 124}, 140: {'episode_index': 140, 'tasks': ['pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate'], 'length': 151}, 141: {'episode_index': 141, 'tasks': ['pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate'], 'length': 147}, 142: {'episode_index': 142, 'tasks': ['pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate'], 'length': 142}, 143: {'episode_index': 143, 'tasks': ['pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate'], 'length': 169}, 144: {'episode_index': 144, 'tasks': ['pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate'], 'length': 149}, 145: {'episode_index': 145, 'tasks': ['pick up the black bowl on the ramekin and place it on the plate'], 'length': 98}, 146: {'episode_index': 146, 'tasks': ['pick up the black bowl on the ramekin and place it on the plate'], 'length': 120}, 147: {'episode_index': 147, 'tasks': ['pick up the black bowl on the ramekin and place it on the plate'], 'length': 117}, 148: {'episode_index': 148, 'tasks': ['pick up the black bowl on the ramekin and place it on the plate'], 'length': 102}, 149: {'episode_index': 149, 'tasks': ['pick up the black bowl on the ramekin and place it on the plate'], 'length': 119}, 150: {'episode_index': 150, 'tasks': ['pick up the tomato sauce and place it in the basket'], 'length': 136}, 151: {'episode_index': 151, 'tasks': ['pick up the tomato sauce and place it in the basket'], 'length': 131}, 152: {'episode_index': 152, 'tasks': ['pick up the tomato sauce and place it in the basket'], 'length': 180}, 153: {'episode_index': 153, 'tasks': ['pick up the tomato sauce and place it in the basket'], 'length': 132}, 154: {'episode_index': 154, 'tasks': ['pick up the tomato sauce and place it in the basket'], 'length': 183}, 155: {'episode_index': 155, 'tasks': ['pick up the ketchup and place it in the basket'], 'length': 136}, 156: {'episode_index': 156, 'tasks': ['pick up the ketchup and place it in the basket'], 'length': 142}, 157: {'episode_index': 157, 'tasks': ['pick up the ketchup and place it in the basket'], 'length': 137}, 158: {'episode_index': 158, 'tasks': ['pick up the ketchup and place it in the basket'], 'length': 141}, 159: {'episode_index': 159, 'tasks': ['pick up the ketchup and place it in the basket'], 'length': 159}, 160: {'episode_index': 160, 'tasks': ['pick up the milk and place it in the basket'], 'length': 126}, 161: {'episode_index': 161, 'tasks': ['pick up the milk and place it in the basket'], 'length': 143}, 162: {'episode_index': 162, 'tasks': ['pick up the milk and place it in the basket'], 'length': 133}, 163: {'episode_index': 163, 'tasks': ['pick up the milk and place it in the basket'], 'length': 143}, 164: {'episode_index': 164, 'tasks': ['pick up the milk and place it in the basket'], 'length': 175}, 165: {'episode_index': 165, 'tasks': ['pick up the alphabet soup and place it in the basket'], 'length': 153}, 166: {'episode_index': 166, 'tasks': ['pick up the alphabet soup and place it in the basket'], 'length': 151}, 167: {'episode_index': 167, 'tasks': ['pick up the alphabet soup and place it in the basket'], 'length': 155}, 168: {'episode_index': 168, 'tasks': ['pick up the alphabet soup and place it in the basket'], 'length': 154}, 169: {'episode_index': 169, 'tasks': ['pick up the alphabet soup and place it in the basket'], 'length': 160}, 170: {'episode_index': 170, 'tasks': ['pick up the orange juice and place it in the basket'], 'length': 142}, 171: {'episode_index': 171, 'tasks': ['pick up the orange juice and place it in the basket'], 'length': 125}, 172: {'episode_index': 172, 'tasks': ['pick up the orange juice and place it in the basket'], 'length': 125}, 173: {'episode_index': 173, 'tasks': ['pick up the orange juice and place it in the basket'], 'length': 148}, 174: {'episode_index': 174, 'tasks': ['pick up the orange juice and place it in the basket'], 'length': 126}, 175: {'episode_index': 175, 'tasks': ['pick up the cream cheese and place it in the basket'], 'length': 139}, 176: {'episode_index': 176, 'tasks': ['pick up the cream cheese and place it in the basket'], 'length': 164}, 177: {'episode_index': 177, 'tasks': ['pick up the cream cheese and place it in the basket'], 'length': 133}, 178: {'episode_index': 178, 'tasks': ['pick up the cream cheese and place it in the basket'], 'length': 116}, 179: {'episode_index': 179, 'tasks': ['pick up the cream cheese and place it in the basket'], 'length': 153}, 180: {'episode_index': 180, 'tasks': ['pick up the salad dressing and place it in the basket'], 'length': 133}, 181: {'episode_index': 181, 'tasks': ['pick up the salad dressing and place it in the basket'], 'length': 134}, 182: {'episode_index': 182, 'tasks': ['pick up the salad dressing and place it in the basket'], 'length': 122}, 183: {'episode_index': 183, 'tasks': ['pick up the salad dressing and place it in the basket'], 'length': 128}, 184: {'episode_index': 184, 'tasks': ['pick up the salad dressing and place it in the basket'], 'length': 216}, 185: {'episode_index': 185, 'tasks': ['pick up the bbq sauce and place it in the basket'], 'length': 150}, 186: {'episode_index': 186, 'tasks': ['pick up the bbq sauce and place it in the basket'], 'length': 175}, 187: {'episode_index': 187, 'tasks': ['pick up the bbq sauce and place it in the basket'], 'length': 133}, 188: {'episode_index': 188, 'tasks': ['pick up the bbq sauce and place it in the basket'], 'length': 132}, 189: {'episode_index': 189, 'tasks': ['pick up the bbq sauce and place it in the basket'], 'length': 172}, 190: {'episode_index': 190, 'tasks': ['pick up the butter and place it in the basket'], 'length': 152}, 191: {'episode_index': 191, 'tasks': ['pick up the butter and place it in the basket'], 'length': 146}, 192: {'episode_index': 192, 'tasks': ['pick up the butter and place it in the basket'], 'length': 161}, 193: {'episode_index': 193, 'tasks': ['pick up the butter and place it in the basket'], 'length': 165}, 194: {'episode_index': 194, 'tasks': ['pick up the butter and place it in the basket'], 'length': 144}, 195: {'episode_index': 195, 'tasks': ['pick up the chocolate pudding and place it in the basket'], 'length': 158}, 196: {'episode_index': 196, 'tasks': ['pick up the chocolate pudding and place it in the basket'], 'length': 162}, 197: {'episode_index': 197, 'tasks': ['pick up the chocolate pudding and place it in the basket'], 'length': 164}, 198: {'episode_index': 198, 'tasks': ['pick up the chocolate pudding and place it in the basket'], 'length': 154}, 199: {'episode_index': 199, 'tasks': ['pick up the chocolate pudding and place it in the basket'], 'length': 224}}\n"
     ]
    }
   ],
   "source": [
    "if cfg.wandb.enable and cfg.wandb.project:\n",
    "    wandb_logger = WandBLogger(cfg)\n",
    "else:\n",
    "    wandb_logger = None\n",
    "    logging.info(colored(\"Logs will be saved locally.\", \"yellow\", attrs=[\"bold\"]))\n",
    "\n",
    "if cfg.seed is not None:\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "device = get_safe_torch_device(cfg.policy.device, log=True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "logging.info(\"Creating dataset\")\n",
    "dataset = make_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment used for evaluating checkpoints during training on simulation data.\n",
    "# On real-world data, no need to create an environment as evaluations are done outside train.py,\n",
    "# using the eval.py instead, with gym_dora environment and dora-rs.\n",
    "eval_env = None\n",
    "if cfg.eval_freq > 0 and cfg.env is not None:\n",
    "    logging.info(\"Creating env\")\n",
    "    eval_env = make_env(cfg.env, n_envs=cfg.eval.batch_size, use_async_envs=cfg.eval.use_async_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-30 13:04:09 3098919642.py:2 Creating policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained policy\n",
      "Loading weights from local directory\n"
     ]
    }
   ],
   "source": [
    "cfg.policy.pretrained_path = \"/pfss/mlde/workspaces/mlde_wsp_PI_Hauschild/VLA/temp/baseline\"\n",
    "logging.info(\"Creating policy\")\n",
    "policy = make_policy(\n",
    "    cfg=cfg.policy,\n",
    "    ds_meta=dataset.meta,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-30 13:07:32 3160407189.py:2 Creating policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained policy\n",
      "Loading weights from local directory\n"
     ]
    }
   ],
   "source": [
    "cfg.policy.pretrained_path = \"../outputs/train/2025-12-29/15-41-19_libero_10%_imi_v8_v3_10-_mh_synthetic_20ep_2_hard_task+5ep_all_normal_task_PRETRAINING_VLM/checkpoints/050000/pretrained_model\"\n",
    "logging.info(\"Creating policy\")\n",
    "mypolicy = make_policy(\n",
    "    cfg=cfg.policy,\n",
    "    ds_meta=dataset.meta,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All parameters identical!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "m1 = policy.model.paligemma_with_expert.gemma_expert\n",
    "m2 = mypolicy.model.paligemma_with_expert.gemma_expert\n",
    "\n",
    "sd1 = m1.state_dict()\n",
    "sd2 = m2.state_dict()\n",
    "\n",
    "assert sd1.keys() == sd2.keys(), \"State dict keys mismatch!\"\n",
    "\n",
    "all_equal = True\n",
    "for k in sd1:\n",
    "    if not torch.equal(sd1[k], sd2[k]):\n",
    "        all_equal = False\n",
    "        print(f\"❌ Mismatch at {k}\")\n",
    "        break\n",
    "\n",
    "print(\"✅ All parameters identical!\" if all_equal else \"⚠️ Parameters differ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model.paligemma_with_expert.gemma_expert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): None\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=257152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypolicy.model.paligemma_with_expert.gemma_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:53:04 2797565836.py:1 Creating optimizer and scheduler\n",
      "INFO 2025-11-21 08:53:04 797565836.py:13 \u001b[1m\u001b[33mOutput dir:\u001b[0m outputs/train/2025-11-21/08-52-19_pi0\n",
      "INFO 2025-11-21 08:53:04 797565836.py:16 cfg.steps=100000 (100K)\n",
      "INFO 2025-11-21 08:53:04 797565836.py:17 dataset.num_frames=133851 (134K)\n",
      "INFO 2025-11-21 08:53:04 797565836.py:18 dataset.num_episodes=800\n",
      "INFO 2025-11-21 08:53:04 797565836.py:19 num_learnable_params=3419097505 (3B)\n",
      "INFO 2025-11-21 08:53:04 797565836.py:20 num_total_params=3846778283 (4B)\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Creating optimizer and scheduler\")\n",
    "optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, policy)\n",
    "grad_scaler = GradScaler(device.type, enabled=cfg.policy.use_amp)\n",
    "\n",
    "step = 0  # number of policy updates (forward + backward + optim)\n",
    "\n",
    "if cfg.resume:\n",
    "    step, optimizer, lr_scheduler = load_training_state(cfg.checkpoint_path, optimizer, lr_scheduler)\n",
    "\n",
    "num_learnable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
    "num_total_params = sum(p.numel() for p in policy.parameters())\n",
    "\n",
    "logging.info(colored(\"Output dir:\", \"yellow\", attrs=[\"bold\"]) + f\" {cfg.output_dir}\")\n",
    "if cfg.env is not None:\n",
    "    logging.info(f\"{cfg.env.task=}\")\n",
    "logging.info(f\"{cfg.steps=} ({format_big_number(cfg.steps)})\")\n",
    "logging.info(f\"{dataset.num_frames=} ({format_big_number(dataset.num_frames)})\")\n",
    "logging.info(f\"{dataset.num_episodes=}\")\n",
    "logging.info(f\"{num_learnable_params=} ({format_big_number(num_learnable_params)})\")\n",
    "logging.info(f\"{num_total_params=} ({format_big_number(num_total_params)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why not fully finetune, which part is freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if hasattr(cfg.policy, \"drop_n_last_frames\"):\n",
    "    shuffle = False\n",
    "    sampler = EpisodeAwareSampler(\n",
    "        dataset.episode_data_index,\n",
    "        drop_n_last_frames=cfg.policy.drop_n_last_frames,\n",
    "        shuffle=True,\n",
    "    )\n",
    "else:\n",
    "    shuffle = True\n",
    "    sampler = None\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=cfg.num_workers,\n",
    "    batch_size=3,\n",
    "    shuffle=shuffle,\n",
    "    sampler=sampler,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=False,\n",
    ")\n",
    "dl_iter = cycle(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'format_kwargs'={'transform': <function hf_transform_to_torch at 0x152831e75a20>} of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING 2025-11-21 08:58:29 gerprint.py:258 Parameter 'format_kwargs'={'transform': <function hf_transform_to_torch at 0x152831e75a20>} of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "sample = next(dl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['observation.images.image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(\n",
    "    train_metrics: MetricsTracker,\n",
    "    policy: PreTrainedPolicy,\n",
    "    batch: Any,\n",
    "    optimizer: Optimizer,\n",
    "    grad_clip_norm: float,\n",
    "    grad_scaler: GradScaler,\n",
    "    lr_scheduler=None,\n",
    "    use_amp: bool = False,\n",
    "    lock=None,\n",
    ") -> tuple[MetricsTracker, dict]:\n",
    "    start_time = time.perf_counter()\n",
    "    device = get_device_from_parameters(policy)\n",
    "    policy.train()\n",
    "    with torch.autocast(device_type=device.type) if use_amp else nullcontext():\n",
    "        loss, output_dict = policy.forward(batch)\n",
    "        # TODO(rcadene): policy.unnormalize_outputs(out_dict)\n",
    "    grad_scaler.scale(loss).backward()\n",
    "\n",
    "    # Unscale the gradient of the optimizer's assigned params in-place **prior to gradient clipping**.\n",
    "    grad_scaler.unscale_(optimizer)\n",
    "\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "        policy.parameters(),\n",
    "        grad_clip_norm,\n",
    "        error_if_nonfinite=False,\n",
    "    )\n",
    "\n",
    "    # Optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n",
    "    # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n",
    "    with lock if lock is not None else nullcontext():\n",
    "        grad_scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration.\n",
    "    grad_scaler.update()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step through pytorch scheduler at every batch instead of epoch\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    if has_method(policy, \"update\"):\n",
    "        # To possibly update an internal buffer (for instance an Exponential Moving Average like in TDMPC).\n",
    "        policy.update()\n",
    "\n",
    "    train_metrics.loss = loss.item()\n",
    "    train_metrics.grad_norm = grad_norm.item()\n",
    "    train_metrics.lr = optimizer.param_groups[0][\"lr\"]\n",
    "    train_metrics.update_s = time.perf_counter() - start_time\n",
    "    return train_metrics, output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:55:38 085197248.py:15 Start offline training on a fixed dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:55:48 085197248.py:56 step:40 smpl:40 ep:0 epch:0.00 loss:6.846 grdn:247.396 lr:3.1e-06 updt_s:0.482 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1520, device='cuda:0') tensor(0.5869, device='cuda:0')\n",
      "tensor(-0.3402, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:55:58 085197248.py:56 step:60 smpl:60 ep:0 epch:0.00 loss:3.874 grdn:110.192 lr:5.1e-06 updt_s:0.483 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4230, device='cuda:0') tensor(0.5324, device='cuda:0')\n",
      "tensor(-0.6348, device='cuda:0') tensor(0.1473, device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:56:08 085197248.py:56 step:80 smpl:80 ep:0 epch:0.00 loss:3.268 grdn:67.777 lr:7.1e-06 updt_s:0.485 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2414, device='cuda:0') tensor(0.5953, device='cuda:0')\n",
      "tensor(-0.6482, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:56:18 085197248.py:56 step:100 smpl:100 ep:1 epch:0.00 loss:3.202 grdn:59.479 lr:9.1e-06 updt_s:0.484 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4188, device='cuda:0') tensor(0.8002, device='cuda:0')\n",
      "tensor(-0.2196, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:56:29 085197248.py:56 step:120 smpl:120 ep:1 epch:0.00 loss:2.833 grdn:68.305 lr:1.1e-05 updt_s:0.486 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8023, device='cuda:0') tensor(1.1237, device='cuda:0')\n",
      "tensor(-0.8009, device='cuda:0') tensor(0.7982, device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:56:39 085197248.py:56 step:140 smpl:140 ep:1 epch:0.00 loss:2.379 grdn:106.243 lr:1.3e-05 updt_s:0.482 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9062, device='cuda:0') tensor(0.8237, device='cuda:0')\n",
      "tensor(-0.2170, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:56:49 085197248.py:56 step:160 smpl:160 ep:1 epch:0.00 loss:2.194 grdn:108.390 lr:1.5e-05 updt_s:0.481 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2207, device='cuda:0') tensor(0.7120, device='cuda:0')\n",
      "tensor(-0.6777, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:56:59 085197248.py:56 step:180 smpl:180 ep:1 epch:0.00 loss:1.818 grdn:97.620 lr:1.7e-05 updt_s:0.479 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7512, device='cuda:0') tensor(0.7877, device='cuda:0')\n",
      "tensor(-0.8598, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:57:09 085197248.py:56 step:200 smpl:200 ep:1 epch:0.00 loss:2.326 grdn:143.076 lr:1.9e-05 updt_s:0.479 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8737, device='cuda:0') tensor(0.3468, device='cuda:0')\n",
      "tensor(-0.1848, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:57:19 085197248.py:56 step:220 smpl:220 ep:1 epch:0.00 loss:2.013 grdn:126.831 lr:2.1e-05 updt_s:0.484 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9542, device='cuda:0') tensor(0.6053, device='cuda:0')\n",
      "tensor(-0.3964, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:57:29 085197248.py:56 step:240 smpl:240 ep:1 epch:0.00 loss:1.854 grdn:94.315 lr:2.3e-05 updt_s:0.480 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7152, device='cuda:0') tensor(0.8818, device='cuda:0')\n",
      "tensor(-0.7955, device='cuda:0') tensor(0.9268, device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:57:39 085197248.py:56 step:260 smpl:260 ep:2 epch:0.00 loss:1.985 grdn:80.669 lr:2.5e-05 updt_s:0.481 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6952, device='cuda:0') tensor(0.9017, device='cuda:0')\n",
      "tensor(-0.5813, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:57:48 085197248.py:56 step:280 smpl:280 ep:2 epch:0.00 loss:2.031 grdn:72.695 lr:2.7e-05 updt_s:0.477 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2392, device='cuda:0') tensor(0.8447, device='cuda:0')\n",
      "tensor(-0.7500, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:57:58 085197248.py:56 step:300 smpl:300 ep:2 epch:0.00 loss:1.888 grdn:59.250 lr:2.9e-05 updt_s:0.478 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1268, device='cuda:0') tensor(1.1937, device='cuda:0')\n",
      "tensor(-0.9054, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:58:08 085197248.py:56 step:320 smpl:320 ep:2 epch:0.00 loss:2.017 grdn:71.680 lr:3.1e-05 updt_s:0.478 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5466, device='cuda:0') tensor(0.6744, device='cuda:0')\n",
      "tensor(-0.6027, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:58:18 085197248.py:56 step:340 smpl:340 ep:2 epch:0.00 loss:1.906 grdn:59.301 lr:3.3e-05 updt_s:0.478 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4432, device='cuda:0') tensor(0.5735, device='cuda:0')\n",
      "tensor(-0.6402, device='cuda:0') tensor(1., device='cuda:0')\n",
      "ductop1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-11-21 08:58:28 085197248.py:56 step:360 smpl:360 ep:2 epch:0.00 loss:2.122 grdn:40.127 lr:3.5e-05 updt_s:0.487 data_s:0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7838, device='cuda:0') tensor(0.8872, device='cuda:0')\n",
      "tensor(-0.1854, device='cuda:0') tensor(1., device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     23\u001b[0m         batch[key] \u001b[38;5;241m=\u001b[39m batch[key]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m train_tracker, output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_clip_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Note: eval and checkpoint happens *after* the `step`th training update has completed, so we\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# increment `step` here.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m, in \u001b[0;36mupdate_policy\u001b[0;34m(train_metrics, policy, batch, optimizer, grad_clip_norm, grad_scaler, lr_scheduler, use_amp, lock)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Optimizer's gradients are already unscaled, so scaler.step does not unscale them,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# although it still skips optimizer.step() if the gradients contain infs or NaNs.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lock \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mgrad_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Updates the scale for next iteration.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m grad_scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/amp/grad_scaler.py:388\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:133\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    132\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/optimizer.py:81\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 81\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/optimizer.py:149\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/adam.py:949\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 949\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/extern/kisski/kisski-umg-fairpact-2/dir.project/miniconda3/envs/pitorch_dino/lib/python3.10/site-packages/torch/optim/adam.py:691\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[1;32m    689\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[0;32m--> 691\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta2, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "policy.train()\n",
    "\n",
    "train_metrics = {\n",
    "    \"loss\": AverageMeter(\"loss\", \":.3f\"),\n",
    "    \"grad_norm\": AverageMeter(\"grdn\", \":.3f\"),\n",
    "    \"lr\": AverageMeter(\"lr\", \":0.1e\"),\n",
    "    \"update_s\": AverageMeter(\"updt_s\", \":.3f\"),\n",
    "    \"dataloading_s\": AverageMeter(\"data_s\", \":.3f\"),\n",
    "}\n",
    "\n",
    "train_tracker = MetricsTracker(\n",
    "    cfg.batch_size, dataset.num_frames, dataset.num_episodes, train_metrics, initial_step=step\n",
    ")\n",
    "\n",
    "logging.info(\"Start offline training on a fixed dataset\")\n",
    "for _ in range(step, cfg.steps):\n",
    "    start_time = time.perf_counter()\n",
    "    batch = next(dl_iter)\n",
    "    train_tracker.dataloading_s = time.perf_counter() - start_time\n",
    "\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch[key] = batch[key].to(device, non_blocking=True)\n",
    "\n",
    "    train_tracker, output_dict = update_policy(\n",
    "        train_tracker,\n",
    "        policy,\n",
    "        batch,\n",
    "        optimizer,\n",
    "        cfg.optimizer.grad_clip_norm,\n",
    "        grad_scaler=grad_scaler,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        use_amp=cfg.policy.use_amp,\n",
    "    )\n",
    "\n",
    "    # Note: eval and checkpoint happens *after* the `step`th training update has completed, so we\n",
    "    # increment `step` here.\n",
    "    step += 1\n",
    "    train_tracker.step()\n",
    "    is_log_step = cfg.log_freq > 0 and step % cfg.log_freq == 0\n",
    "    is_saving_step = step % cfg.save_freq == 0 or step == cfg.steps\n",
    "    is_eval_step = cfg.eval_freq > 0 and step % cfg.eval_freq == 0\n",
    "\n",
    "    if is_log_step:\n",
    "        ###### compute l1 loss\n",
    "        print(\"ductop1\")\n",
    "        pred = policy.select_action_chunk(batch)\n",
    "        print(pred[:,-1].min(), pred[:,-1].max())\n",
    "        pred[:,:,-1] = torch.where(pred[:,:,-1] > 0, torch.tensor(1.0, device=device), torch.tensor(-1.0, device=device))\n",
    "        gt = batch[ACTION]\n",
    "        print(gt[:,-1].min(), gt[:,-1].max())\n",
    "        assert pred.shape == gt.shape, f\"Pred shape {pred.shape} does not match GT shape {gt.shape}\"\n",
    "        l1_infer_loss = torch.nn.functional.l1_loss(pred, gt)\n",
    "        ######\n",
    "\n",
    "        logging.info(train_tracker)\n",
    "        if wandb_logger:\n",
    "            wandb_log_dict = train_tracker.to_dict()\n",
    "            if output_dict:\n",
    "                wandb_log_dict.update(output_dict)\n",
    "            wandb_logger.log_dict(wandb_log_dict, step)\n",
    "        train_tracker.reset_averages()\n",
    "\n",
    "    if cfg.save_checkpoint and is_saving_step:\n",
    "        logging.info(f\"Checkpoint policy after step {step}\")\n",
    "        checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, step)\n",
    "        save_checkpoint(checkpoint_dir, step, cfg, policy, optimizer, lr_scheduler)\n",
    "        update_last_checkpoint(checkpoint_dir)\n",
    "        if wandb_logger:\n",
    "            wandb_logger.log_policy(checkpoint_dir)\n",
    "        print(wandb_logger)\n",
    "\n",
    "    if cfg.env and is_eval_step:\n",
    "        step_id = get_step_identifier(step, cfg.steps)\n",
    "        logging.info(f\"Eval policy at step {step}\")\n",
    "        with (\n",
    "            torch.no_grad(),\n",
    "            torch.autocast(device_type=device.type) if cfg.policy.use_amp else nullcontext(),\n",
    "        ):\n",
    "            eval_info = eval_policy(\n",
    "                eval_env,\n",
    "                policy,\n",
    "                cfg.eval.n_episodes,\n",
    "                videos_dir=cfg.output_dir / \"eval\" / f\"videos_step_{step_id}\",\n",
    "                max_episodes_rendered=4,\n",
    "                start_seed=cfg.seed,\n",
    "            )\n",
    "\n",
    "        eval_metrics = {\n",
    "            \"avg_sum_reward\": AverageMeter(\"∑rwrd\", \":.3f\"),\n",
    "            \"pc_success\": AverageMeter(\"success\", \":.1f\"),\n",
    "            \"eval_s\": AverageMeter(\"eval_s\", \":.3f\"),\n",
    "        }\n",
    "        eval_tracker = MetricsTracker(\n",
    "            cfg.batch_size, dataset.num_frames, dataset.num_episodes, eval_metrics, initial_step=step\n",
    "        )\n",
    "        eval_tracker.eval_s = eval_info[\"aggregated\"].pop(\"eval_s\")\n",
    "        eval_tracker.avg_sum_reward = eval_info[\"aggregated\"].pop(\"avg_sum_reward\")\n",
    "        eval_tracker.pc_success = eval_info[\"aggregated\"].pop(\"pc_success\")\n",
    "        logging.info(eval_tracker)\n",
    "        if wandb_logger:\n",
    "            wandb_log_dict = {**eval_tracker.to_dict(), **eval_info}\n",
    "            wandb_logger.log_dict(wandb_log_dict, step, mode=\"eval\")\n",
    "            wandb_logger.log_video(eval_info[\"video_paths\"][0], step, mode=\"eval\")\n",
    "\n",
    "if eval_env:\n",
    "    eval_env.close()\n",
    "logging.info(\"End of training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
